# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
"""Metrics computation."""

from collections import Counter
from functools import reduce
from operator import add
from pprint import pformat
from typing import List

import torch
import wandb


LBR = "["
RBR = "]"
IN = "IN:"
SL = "SL:"


class Tree:
    """TOP format tree object."""

    def __init__(self, entity, subtrees: List = None):
        """Make a tree node with value entity and subtrees.

        if subtrees is None, .subtrees attribute will be empty list

        Args:
            entity: intent/slot value, e.g. IN:INTENT
            subtrees: list of Tree objects
        """
        self.entity = entity
        self.subtrees = subtrees
        if subtrees is None:
            self.subtrees = []

        # for per-class metrics
        self._counts = Counter([entity])
        self._len = 1

        if len(self.subtrees) > 0:
            self._len += sum(map(len, self.subtrees))
            self._counts += reduce(add, (s._counts for s in self.subtrees))

        # for __repr__ and __eq__
        self._dict_repr = {self.entity: [s._dict_repr for s in self.subtrees]}

    def __repr__(self):
        return pformat(self._dict_repr)

    def __eq__(self, other):
        if isinstance(other, dict):
            return self._dict_repr == other
        if isinstance(other, Tree):
            return self._dict_repr == other._dict_repr
        raise ValueError(type(other))

    def __len__(self):
        return self._len

    def get_size(self, classes=None):
        """Get the number of nodes with node.entity in classes.

        Args:
            classes: optional, a iterable of classes to consider when computing the size

        Returns:
            if classes argument is not specified, returns the total number of nodes
            (same as __len__)
            if classes argument is specified, returns only the count of nodes corresponding to classes
        """
        if classes is None:
            return self.__len__()

        _size = sum(self._counts.get(c, 0) for c in classes)
        return _size

    @property
    def counts(self):
        return self._counts

    @classmethod
    def from_tokens(cls, tokens, return_index=False, inside_slot=False):
        """Builds a parsing tree for labeled bracketing score computation.

        The tree is build until the last ] symbol, everything after it is ignored

        Args:
            tokens: list of tokens
            return_index: used in recursion to provide token index

        Returns:
            Tree object, if return_index == False
            tuple (Tree, index), if return_index == True

        Raises:
            ValueError, if tokens do not represent a valid tree
        """
        # every tree should start with
        # [ ENTITY_TYPE: ENTITY
        if len(tokens) < 3 or tokens[0] != LBR:
            raise ValueError(f"Tree starts with {tokens[:4]}")

        entity_type = tokens[1]

        # ignore invalid subtrees
        if entity_type not in [IN, SL]:
            raise ValueError(f"Tree starts with {tokens[:4]}")

        entity = entity_type + tokens[2]  # e.g. IN:INTENT

        subtrees = []
        slot_value_tokens = []

        i = 3
        inside_slot = inside_slot or entity_type == SL
        while i < len(tokens):
            token = tokens[i]

            # ignore non-slot values
            # e.g. ignore "go stuff in" [IN:STUFF Do stuff]
            if not inside_slot and token not in [LBR, RBR]:
                i += 1
                continue

            # LBR starts a new subtree
            if token == LBR:
                subtree, j = cls.from_tokens(
                    tokens[i:], return_index=True, inside_slot=inside_slot
                )

                if slot_value_tokens:
                    subtrees.append(Tree(" ".join(slot_value_tokens)))
                    slot_value_tokens = []

                subtrees.append(subtree)
                i += j

                continue

            # RBR ends the tree, merge slot values into a single leaf if any
            # e.g. "stuff value" becomes a single leaf in [IN:GET_STUFF [SL:STUF_VALUE stuff value]]
            if token == RBR:
                if slot_value_tokens:
                    subtrees.append(Tree(" ".join(slot_value_tokens)))
                    slot_value_tokens = []

                i += 1
                break

            # if the token is not a special symbol and inside SL: bracket (probably, nested)
            slot_value_tokens.append(token)
            i += 1

        tree = Tree(entity, subtrees)

        if return_index:
            return tree, i

        return tree

    def to_tokens(self):
        if not self.subtrees:
            return self.entity

        return f"[{self.entity} {self.subtrees_to_tokens()}]"

    def subtrees_to_tokens(self):
        return " ".join([s.to_tokens() for s in self.subtrees])


# Main function


def get_metrics(
    pred_tokens, true_tokens, monitor_classes, prefix, schema_tokenizer, do_each=False
):
    """Compute exact_match and tree-based metrics

    Apply prefix to all keys.
    The main purpuse of this function is to unify evaluation in PointerModule and cli_utils.evaluate_model()

    Args:
        pred_tokens: List[List[str]]
        true_tokens: List[List[str]]
        monitor_classes: List[str]
        prefix: str, will be appended to all return dict keys
        schema_tokenizer: TopSchemaTokenizer
        do_each: bool, if False compute tree path metrics only for monitor_classes[0] and overall
            if True compute tree path metrics for all monitor_classes and overall

    Returns:
        dictionary with keys
            {prefix}_{score_name}
            {prefix}_new_classes_{score_name}
            cls/{prefix}_{monitor_classes[i]}_{score_name}; if do_each=False then only i == 0, else for each class
        for each score_name - key from get_tree_path_scores output dictionary
    """
    exact_match = sum(int(str(p) == str(l)) for p, l in zip(pred_tokens, true_tokens))
    exact_match /= len(true_tokens)

    tree_metrics = get_tree_path_metrics(
        pred_tokens, true_tokens, monitor_classes, prefix, do_each
    )

    pred_strs = [schema_tokenizer.detokenize(p) for p in pred_tokens]
    true_strs = [schema_tokenizer.detokenize(p) for p in true_tokens]

    exact_match_str = sum(int(p == t) for p, t in zip(pred_strs, true_strs)) / len(true_strs)

    log_dict = {
        f"{prefix}_exact_match": exact_match,
        f"{prefix}_exact_match_str": exact_match_str,
        **tree_metrics,
    }

    return log_dict


# Tree path scores


def get_tree_path_metrics(pred_tokens, true_tokens, monitor_classes, prefix, do_each=False):
    """Get metrics for all classes, for monitor classes and for monitor_classes[0].

    Apply prefix to all keys.

    Args:
        pred_tokens: List[List[str]]
        true_tokens: List[List[str]]
        monitor_classes: List[str]
        prefix: str, will be appended to all return dict keys
        do_each: bool, if False compute tree path metrics only for monitor_classes[0] and overall
            if True compute tree path metrics for all monitor_classes and overall

    Returns:
        dictionary with keys
            {prefix}_{score_name}
            {prefix}_new_classes_{score_name}
            cls/{prefix}_{monitor_classes[i]}_{score_name}, if do_each=False then i is only == 0
        for each score_name - key from get_tree_path_scores output dictionary
    """

    tree_path_scores = get_tree_path_scores(pred_tokens=pred_tokens, true_tokens=true_tokens)
    tree_path_scores = {f"{prefix}_{k}": v for k, v in tree_path_scores.items()}

    if monitor_classes is not None:
        _new_classes_scores = get_tree_path_scores(
            pred_tokens=pred_tokens, true_tokens=true_tokens, classes=monitor_classes
        )
        _new_classes_scores = {
            f"{prefix}_new_classes_{k}": v for k, v in _new_classes_scores.items()
        }
        tree_path_scores.update(_new_classes_scores)

        for i, class_ in enumerate(monitor_classes):
            if i > 0 and not do_each:
                break

            _class_score = get_tree_path_scores(
                pred_tokens=pred_tokens, true_tokens=true_tokens, classes=[class_]
            )
            _class_score = {f"cls/{prefix}_{class_}_{k}": v for k, v in _class_score.items()}
            tree_path_scores.update(_class_score)

    return tree_path_scores


def get_tree_path_scores(pred_tokens, true_tokens, classes=None):
    """
    Args:
        pred_tokens: list of lists of tokens
        true_tokens: list of lists of tokens

    Returns:
        dictionary with keys
            predicted_paths
            expected_paths
            tree_path_precision
            tree_path_recall
            tree_path_f1
    """
    pred_paths_lst, true_paths_lst = [], []

    for pred, true in zip(pred_tokens, true_tokens):
        try:
            pred_paths = _get_paths_with_values(Tree.from_tokens(pred))
        except ValueError:
            pred_paths = dict()

        # we need to build true tree even if pred tree is not valid to compute correct n_expected
        true_paths = _get_paths_with_values(Tree.from_tokens(true))

        pred_paths_lst.append(pred_paths)
        true_paths_lst.append(true_paths)

    true_positives = 0
    n_predicted = 0
    n_expected = 0

    for pred_paths, true_paths in zip(pred_paths_lst, true_paths_lst):
        if classes is None:
            n_expected += len(true_paths)
            n_predicted += len(pred_paths)
        else:
            n_expected += len([p for p in true_paths if any(c in p for c in classes)])
            n_predicted += len([p for p in pred_paths if any(c in p for c in classes)])

        true_positives += _get_tree_path_matches(pred_paths, true_paths, classes)

    precision = 0
    if n_predicted > 0:
        precision = true_positives / n_predicted

    recall = 0
    if n_expected > 0:
        recall = true_positives / n_expected

    f1 = 0
    if precision + recall > 0:
        f1 = 2 * precision * recall / (precision + recall)

    return {
        "tree_path_precision": precision,
        "tree_path_recall": recall,
        "tree_path_f1": f1,
    }


def _get_paths_with_values(tree) -> dict:
    """Go over the tree and return all slot values with the slot names.

    Slot names include paths to this slot. E.g. IN1.SL1: sl1_value.
    Intents and slot without values have None as a value.

    Args:
        tree: Tree object
    """
    is_special = SL in tree.entity or IN in tree.entity
    if not is_special:
        return {}

    if tree.subtrees is None:
        return {tree.entity: None}

    is_slot = SL in tree.entity
    paths = {}

    if is_slot:
        paths[tree.entity] = tree.subtrees_to_tokens()

    for subtree in tree.subtrees:
        subpaths = _get_paths_with_values(subtree)
        for path, value in subpaths.items():
            paths[tree.entity + "." + path] = value

    return paths


def _get_tree_path_matches(pred_tree_paths, true_tree_paths, classes=None):
    n_matches = 0
    for true_path, true_slot in true_tree_paths.items():
        path_value_match = true_path in pred_tree_paths and pred_tree_paths[true_path] == true_slot
        class_match = classes is None or any(c in true_path for c in classes)

        if path_value_match and class_match:
            n_matches += 1

    return n_matches


# Non-tree metrics: EM, Accuracy, first intent precision


def compute_metrics_from_batch(predictions, labels, masks, stop_tokens):
    """Compute metrics where all predictions, labels and masks are torch.tensor"""
    device = predictions.device

    # correct tokens which are not masked (masked tokens have mask == 0)
    n_correct = ((predictions == labels) & masks.bool()).sum()
    n_total = masks.sum()
    accuracy = n_correct / n_total.float()

    # trauncate until EOS token
    # for exact match we consider all tokens until EOS/PAD
    # this is closer to inference setup when generation stops after EOS/PAD

    def truncate(pred):
        i = 0
        for i, idx in enumerate(pred):
            if idx in stop_tokens:
                break
        return pred[:i]

    truncated_preds = [truncate(p) for p in predictions.detach().unbind()]
    truncated_labels = [truncate(l) for l in labels.detach().unbind()]

    exact_match = sum(
        (p.shape == l.shape and torch.all(p == l))
        for p, l in zip(truncated_preds, truncated_labels)
    )
    if exact_match == 0:
        exact_match = torch.tensor(0.0, device=device)

    exact_match = exact_match.float() / len(truncated_preds)

    # intent is the third token in the sequence
    # [ IN: INTENT ...
    # 0 1   2
    intent_tokens_preds = predictions[:, 2]
    intent_tokens_labels = labels[:, 2]

    first_intent_precision = torch.sum(intent_tokens_preds == intent_tokens_labels) / float(
        intent_tokens_preds.shape[0]
    )

    return {
        "accuracy": accuracy,
        "exact_match": exact_match,
        "first_intent_precision": first_intent_precision,
    }


# Metrics used to evaluate finetuning procedure


def get_outliers_metrics(
    metric_names, initial_metrics, final_metrics, prefix, suffix, metric_weights
):
    """Get aggregate metrics evaluating the change of initial and final performance.

    Metrics are summed with metric_weights for relative metrics and without weights for absolute metrics.
    E.g., if metric_names is a list of positive outliers, this function computes Relative Improvemen
    and the total metric increase on these classes.

    Args:
        metric_names: list of metrics to compute change on
        initial_metrics: dict metric_name:metric_value
        final_metrics: dict metric_name:metric_value
        prefix: string used for naming, see Returns
        suffix: string used for naming, see Returns
        metric_weights: weights used to aggregate relative change

    Returns:
        dict with keys
            {prefix}_outliers
            absolute_{suffix}
            relative_{suffix}
    """
    table = wandb.Table(
        columns=[
            "metric_name",
            "pretrain_mean",
            "pretrain_stdev",
            "final_mean",
            "final_stdev",
            "absolute_improvement",
            "relative_improvement",
        ]
    )

    abs_deltas = {}
    rel_deltas = {}
    digits = 4

    for name in metric_names:
        abs_delta = final_metrics["means"][name] - initial_metrics["means"][name]
        rel_delta = abs_delta / max(initial_metrics["means"][name], 0.001)
        abs_deltas[name] = abs_delta
        rel_deltas[name] = rel_delta

        table.add_data(
            name,
            round(initial_metrics["means"][name], digits),
            round(initial_metrics["stdevs"][name + "_std"], digits),
            round(final_metrics["means"][name], digits),
            round(final_metrics["stdevs"][name + "_std"], digits),
            round(abs_delta, digits),
            round(rel_delta, digits),
        )

    abs_delta_overall = 0
    rel_delta_overall = 0

    if len(abs_deltas) > 0:
        abs_delta_overall = sum(abs_deltas.values())
        rel_delta_overall = sum(metric_weights[name] * v for name, v in rel_deltas.items())

        abs_delta_overall = round(abs_delta_overall, digits)
        rel_delta_overall = round(rel_delta_overall, digits)

    res = {
        f"{prefix}_outliers": table,
        f"absolute_{suffix}": abs_delta_overall,
        f"relative_{suffix}": rel_delta_overall,
    }
    return res
